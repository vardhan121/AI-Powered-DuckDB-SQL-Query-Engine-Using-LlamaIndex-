{"cells":[{"source":["%%capture\n","%pip install duckdb\n","%pip install llama-index\n","%pip install llama-index-vector-stores-duckdb"],"metadata":{"executionCancelledAt":null,"executionTime":20750,"lastExecutedAt":1761295162594,"lastExecutedByKernel":"7dd4c51c-e914-4d68-9474-5684e9e744d9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"%%capture\n%pip install duckdb\n%pip install llama-index\n%pip install llama-index-vector-stores-duckdb","outputsMetadata":{"0":{"height":473,"type":"stream"}},"id":"60e40fac-1210-4423-a0b2-8d9cc8bc3877"},"id":"60e40fac-1210-4423-a0b2-8d9cc8bc3877","cell_type":"code","execution_count":null,"outputs":[]},{"source":["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.vector_stores.duckdb import DuckDBVectorStore\n","from llama_index.core import StorageContext\n","\n","from IPython.display import Markdown, display"],"metadata":{"executionCancelledAt":null,"executionTime":452,"lastExecutedAt":1761295198855,"lastExecutedByKernel":"7dd4c51c-e914-4d68-9474-5684e9e744d9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.core import StorageContext\n\nfrom IPython.display import Markdown, display","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"id":"1ec10e1d-e4dd-4e05-9e53-fcd8dcedfc0f","executionInfo":{"status":"ok","timestamp":1761307248743,"user_tz":-330,"elapsed":33,"user":{"displayName":"harsha vardhan","userId":"16565328507495909914"}}},"id":"1ec10e1d-e4dd-4e05-9e53-fcd8dcedfc0f","cell_type":"code","execution_count":null,"outputs":[]},{"source":["import os\n","from llama_index.llms.openai import OpenAI\n","\n","llm = OpenAI(model=\"gpt-4o\",api_key=\"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\")"],"metadata":{"executionCancelledAt":null,"executionTime":458,"lastExecutedAt":1761295313811,"lastExecutedByKernel":"7dd4c51c-e914-4d68-9474-5684e9e744d9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-4o\",api_key=os.environ[\"OPENAI_API_KEY\"])","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"id":"276cdb3e-bb47-483b-8073-89db35c8df0a","executionInfo":{"status":"ok","timestamp":1761307253316,"user_tz":-330,"elapsed":6,"user":{"displayName":"harsha vardhan","userId":"16565328507495909914"}}},"id":"276cdb3e-bb47-483b-8073-89db35c8df0a","cell_type":"code","execution_count":null,"outputs":[]},{"source":["from llama_index.embeddings.openai import OpenAIEmbedding\n","embed_model = OpenAIEmbedding(\n","    model=\"text-embedding-3-small\",\n",")"],"metadata":{"executionCancelledAt":null,"executionTime":574,"lastExecutedAt":1761295282446,"lastExecutedByKernel":"7dd4c51c-e914-4d68-9474-5684e9e744d9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.embeddings.openai import OpenAIEmbedding\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n)","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"id":"309e624f-c255-42cc-abd9-f90a4013d956","executionInfo":{"status":"ok","timestamp":1761307259071,"user_tz":-330,"elapsed":7,"user":{"displayName":"harsha vardhan","userId":"16565328507495909914"}}},"id":"309e624f-c255-42cc-abd9-f90a4013d956","cell_type":"code","execution_count":null,"outputs":[]},{"source":["from llama_index.core import Settings\n","\n","Settings.llm = llm\n","Settings.embed_model = embed_model"],"metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1718813229330,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.core import Settings\n\nSettings.llm = llm\nSettings.embed_model = embed_model","id":"de1abd89-7459-4b72-8dd8-bb981ed9d363"},"id":"de1abd89-7459-4b72-8dd8-bb981ed9d363","cell_type":"code","execution_count":null,"outputs":[]},{"source":["documents = SimpleDirectoryReader(\"Data\").load_data()"],"metadata":{"executionCancelledAt":null,"executionTime":420,"lastExecutedAt":1761295113489,"lastExecutedByKernel":"7dd4c51c-e914-4d68-9474-5684e9e744d9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"documents = SimpleDirectoryReader(\"Data\").load_data()","id":"a132f2e3-8790-4882-b284-bdf69098dee8"},"id":"a132f2e3-8790-4882-b284-bdf69098dee8","cell_type":"code","execution_count":null,"outputs":[]},{"source":["vector_store = DuckDBVectorStore(database_name = \"datacamp.duckdb\",table_name = \"blog\",persist_dir=\"./\", embed_dim=1536)\n","storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","\n","index = VectorStoreIndex.from_documents(\n","    documents, storage_context=storage_context\n",")"],"metadata":{"executionCancelledAt":null,"executionTime":2826,"lastExecutedAt":1718813262654,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"vector_store = DuckDBVectorStore(database_name = \"datacamp.duckdb\",table_name = \"blog\",persist_dir=\"./\", embed_dim=1536)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)","id":"e88843d2-7c77-447b-bf98-8fef443d22dd"},"id":"e88843d2-7c77-447b-bf98-8fef443d22dd","cell_type":"code","execution_count":null,"outputs":[]},{"source":["import duckdb\n","con = duckdb.connect(\"datacamp.duckdb\")\n","\n","con.execute(\"SHOW ALL TABLES\").fetchdf()"],"metadata":{"executionCancelledAt":null,"executionTime":94,"lastExecutedAt":1718813328987,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import duckdb\ncon = duckdb.connect(\"datacamp.duckdb\")\n\ncon.execute(\"SHOW ALL TABLES\").fetchdf()","outputsMetadata":{"0":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"765aa3c8-6423-40d5-babe-7d7f4b41aeb2","nodeType":"const"}}}},"id":"52616d3e-4e38-47db-9dd3-176dcd9df6e3","outputId":"8ee9556c-f25a-4146-c919-ca4413673192"},"id":"52616d3e-4e38-47db-9dd3-176dcd9df6e3","cell_type":"code","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"database","type":"string"},{"name":"schema","type":"string"},{"name":"name","type":"string"},{"name":"column_names","type":"string"},{"name":"column_types","type":"string"},{"name":"temporary","type":"boolean"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1],"database":["datacamp","datacamp"],"schema":["main","main"],"name":["bank","blog"],"column_names":[["age","job","marital","education","default","housing","loan","contact","month","day_of_week","duration","campaign","pdays","previous","poutcome","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y"],["node_id","text","embedding","metadata_"]],"column_types":[["BIGINT","VARCHAR","VARCHAR","VARCHAR","VARCHAR","VARCHAR","VARCHAR","VARCHAR","VARCHAR","VARCHAR","BIGINT","BIGINT","BIGINT","BIGINT","VARCHAR","DOUBLE","DOUBLE","DOUBLE","DOUBLE","DOUBLE","VARCHAR"],["VARCHAR","VARCHAR","FLOAT[1536]","JSON"]],"temporary":[false,false]}},"total_rows":2,"truncation_type":null},"text/plain":"   database  ... temporary\n0  datacamp  ...     False\n1  datacamp  ...     False\n\n[2 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>database</th>\n      <th>schema</th>\n      <th>name</th>\n      <th>column_names</th>\n      <th>column_types</th>\n      <th>temporary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>datacamp</td>\n      <td>main</td>\n      <td>bank</td>\n      <td>[age, job, marital, education, default, housin...</td>\n      <td>[BIGINT, VARCHAR, VARCHAR, VARCHAR, VARCHAR, V...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>datacamp</td>\n      <td>main</td>\n      <td>blog</td>\n      <td>[node_id, text, embedding, metadata_]</td>\n      <td>[VARCHAR, VARCHAR, FLOAT[1536], JSON]</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":65}]},{"source":["query_engine = index.as_query_engine()\n","response = query_engine.query(\"Who wrote 'GitHub Actions and MakeFile: A Hands-on Introduction'?\")\n","display(Markdown(f\"<b>{response}</b>\"))"],"metadata":{"executionCancelledAt":null,"executionTime":1620,"lastExecutedAt":1718813264276,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who wrote 'GitHub Actions and MakeFile: A Hands-on Introduction'?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))","id":"dc8088b0-dfc0-471d-8d43-bf5b6e7d976e","outputId":"e0942fec-2ea7-44f3-c7bf-4e50a749befd"},"id":"dc8088b0-dfc0-471d-8d43-bf5b6e7d976e","cell_type":"code","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<b>The author of \"GitHub Actions and MakeFile: A Hands-on Introduction\" is Abid Ali Awan.</b>"},"metadata":{}}]},{"source":["from llama_index.core.memory import ChatMemoryBuffer\n","from llama_index.core.chat_engine import CondensePlusContextChatEngine\n","\n","memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n","\n","chat_engine = CondensePlusContextChatEngine.from_defaults(\n","    index.as_retriever(),\n","    memory=memory,\n","    llm=llm\n",")\n","\n","response = chat_engine.chat(\n","    \"What is the easiest way of finetuning the Llama 3 model? Please provide step-by-step instructions.\"\n",")\n","\n","display(Markdown(response.response))"],"metadata":{"executionCancelledAt":null,"executionTime":11370,"lastExecutedAt":1718813275646,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n\nchat_engine = CondensePlusContextChatEngine.from_defaults(\n    index.as_retriever(),\n    memory=memory,\n    llm=llm\n)\n\nresponse = chat_engine.chat(\n    \"What is the easiest way of finetuning the Llama 3 model? Please provide step-by-step instructions.\"\n)\n\ndisplay(Markdown(response.response))","id":"464cdb35-86dd-4a24-8b63-3456e484fd57","outputId":"a7f6a220-4e82-4557-e983-72be5e460ecf"},"id":"464cdb35-86dd-4a24-8b63-3456e484fd57","cell_type":"code","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The easiest way to fine-tune the Llama 3 model involves using the Kaggle Notebook and following a series of steps. Here’s a detailed step-by-step guide based on the provided documents:\n\n### Step-by-Step Instructions for Fine-Tuning Llama 3\n\n1. **Fill Out the Meta Download Form:**\n   - Before you start, you need to fill out the Meta download form with your Kaggle email address. This is necessary to access the Llama 3 model.\n\n2. **Accept the Agreement on Kaggle:**\n   - Go to the Llama 3 model page on Kaggle and accept the agreement. The approval process may take one to two days.\n\n3. **Launch a New Notebook on Kaggle:**\n   - Once you have access, launch a new Notebook on Kaggle.\n\n4. **Add the Llama 3 Model:**\n   - In the Notebook, click the `+ Add Input` button.\n   - Select the `Models` option.\n   - Click on the plus `+` button beside the Llama 3 model.\n   - Select the appropriate framework, variation, and version, and add the model to your Notebook.\n\n5. **Select the GPU Accelerator:**\n   - Go to the `Session` options in the Notebook.\n   - Select the `GPU P100` as an accelerator to ensure you have the necessary computational power for fine-tuning.\n\n6. **Prepare the Dataset:**\n   - For this tutorial, the `ruslanmv/ai-medical-chatbot` dataset is used, which contains 250k dialogues between a patient and a doctor.\n   - Ensure the dataset is accessible in your Kaggle environment.\n\n7. **Fine-Tune the Model:**\n   - Load the dataset into your Notebook.\n   - Use the appropriate scripts and libraries to fine-tune the Llama 3 model on the medical dataset. This typically involves setting up a training loop, defining the model parameters, and running the training process.\n\n8. **Merge the Adapter with the Base Model:**\n   - After fine-tuning, merge the adapter with the base model.\n   - Push the full model to the Hugging Face Hub for easier access and sharing.\n\n9. **Convert the Model Files:**\n   - Convert the model files into the `Llama.cpp GGUF` format to make them compatible with local applications.\n\n10. **Quantize the Model:**\n    - Quantize the GGUF model to reduce its size and improve efficiency.\n    - Push the quantized model file to the Hugging Face Hub.\n\n11. **Use the Fine-Tuned Model Locally:**\n    - Finally, use the fine-tuned model locally with the Jan application, which allows for private and efficient use of the model on your local machine.\n\nBy following these steps, you can fine-tune the Llama 3 model efficiently and prepare it for local use."},"metadata":{}}]},{"source":["response = chat_engine.chat(\n","    \"Could you please provide more details about the Post Fine-Tuning Steps?\"\n",")\n","display(Markdown(response.response))"],"metadata":{"executionCancelledAt":null,"executionTime":12259,"lastExecutedAt":1718813287905,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = chat_engine.chat(\n    \"Could you please provide more details about the Post Fine-Tuning Steps?\"\n)\ndisplay(Markdown(response.response))","outputsMetadata":{"0":{"height":473,"type":"stream"}},"id":"5b841345-e5fb-48b5-b710-cf7d3c70d568","outputId":"03be91e6-9618-4d89-a115-e0cc5582f349"},"id":"5b841345-e5fb-48b5-b710-cf7d3c70d568","cell_type":"code","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Certainly! After you have fine-tuned the Llama 3 model, there are several important steps to ensure the model is ready for local use and optimized for performance. Here are the detailed post fine-tuning steps:\n\n### Post Fine-Tuning Steps\n\n1. **Merge the Adapter with the Base Model:**\n   - **Purpose:** Combining the fine-tuned adapter with the base model ensures that the model incorporates the new knowledge gained during fine-tuning.\n   - **Process:**\n     - Use the appropriate tools and scripts to merge the adapter weights with the base model weights.\n     - This step typically involves loading both the base model and the adapter, then applying the adapter's weights to the base model.\n\n2. **Push the Full Model to Hugging Face Hub:**\n   - **Purpose:** Sharing the model on Hugging Face Hub makes it accessible for further use and collaboration.\n   - **Process:**\n     - Create a repository on Hugging Face Hub if you don't already have one.\n     - Use the `transformers` library or Hugging Face CLI to push the model to the repository.\n     - Ensure you include all necessary files, such as the model weights, configuration files, and tokenizer.\n\n3. **Convert the Model Files into Llama.cpp GGUF Format:**\n   - **Purpose:** Converting the model into the GGUF format makes it compatible with the Llama.cpp framework, which is optimized for local deployment.\n   - **Process:**\n     - Use conversion tools provided by the Llama.cpp framework to transform the model files.\n     - This step may involve specifying the input model format and the desired output format (GGUF).\n\n4. **Quantize the GGUF Model:**\n   - **Purpose:** Quantization reduces the model size and improves inference speed, making it more efficient for local use.\n   - **Process:**\n     - Apply quantization techniques to the GGUF model. This often involves reducing the precision of the model weights (e.g., from 32-bit floating point to 8-bit integers).\n     - Use quantization tools that support the GGUF format to perform this step.\n\n5. **Push the Quantized Model to Hugging Face Hub:**\n   - **Purpose:** Making the quantized model available on Hugging Face Hub ensures that you and others can easily access and use the optimized model.\n   - **Process:**\n     - Similar to pushing the full model, use the Hugging Face CLI or `transformers` library to upload the quantized model files to your repository.\n     - Ensure you update the repository with the new quantized model files and any relevant documentation.\n\n6. **Using the Fine-Tuned Model Locally with Jan Application:**\n   - **Purpose:** The Jan application allows you to run the fine-tuned model on your local machine, providing a private and efficient way to use the model.\n   - **Process:**\n     - Install the Jan application on your local machine.\n     - Download the quantized GGUF model from Hugging Face Hub.\n     - Configure the Jan application to use the downloaded model.\n     - Run the application and start using the fine-tuned model for your specific tasks.\n\nBy following these post fine-tuning steps, you ensure that your fine-tuned Llama 3 model is optimized, accessible, and ready for efficient local deployment."},"metadata":{}}]},{"source":["con.close()"],"metadata":{"executionCancelledAt":null,"executionTime":24,"lastExecutedAt":1718813393398,"lastExecutedByKernel":"bc65c287-9046-4820-ad30-c6119cc467f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"con.close()","id":"b6c61964-6d16-4ee1-80ad-f349f0fe8248"},"id":"b6c61964-6d16-4ee1-80ad-f349f0fe8248","cell_type":"code","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}